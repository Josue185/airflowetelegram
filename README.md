Este projeto demonstra como orquestrar um pipeline simples de dados utilizando Apache Airflow, com execuÃ§Ã£o containerizada via Docker, e recursos adicionais como envio de notificaÃ§Ãµes automÃ¡ticas via Telegram.

ðŸ”§ Tecnologias utilizadas
Apache Airflow 2.9.1 (via Docker)

Python 3.12

Pandas

PyArrow (suporte a arquivos Parquet)

Requests

Dotenv

Telegram Bot API

airflow-projeto/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ tratar_csv_para_parquet.py       # DAG principal
â”‚   â”œâ”€â”€ abrindo_parquet.py               # Leitura e tratamento adicional do Parquet
â”‚   â”œâ”€â”€ dados/                           # CSV de entrada
â”‚   â”‚   â””â”€â”€ dados.csv
â”‚   â”œâ”€â”€ resultados/                      # Arquivos Parquet gerados
â”‚   â””â”€â”€ .env                             # VariÃ¡veis de ambiente sensÃ­veis (TOKEN, CHAT_ID)
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ requirements.txt

Requisitos
Docker e Docker Compose

Bibliotecas Python no container:

pandas

pyarrow

requests

python-dotenv
